
@book{payling_electronic_2023,
	address = {Oxford, UNITED KINGDOM},
	title = {Electronic {Visual} {Music}: {The} {Elements} of {Audiovisual} {Creativity}},
	isbn = {978-1-000-93647-6},
	shorttitle = {Electronic {Visual} {Music}},
	url = {http://ebookcentral.proquest.com/lib/oxy/detail.action?docID=7271293},
	abstract = {Electronic Visual Music is a comprehensive guide to the composition and performance of visual music, and an essential text for those wanting to explore the history, current practice, performance strategies, compositional methodologies and practical techniques for conceiving and creating electronic visual music. Beginning with historical perspectives to inspire the reader to work creatively and develop their own individual style, visual music theory is then discussed in an accessible form, providing a series of strategies for implementing ideas. Including interviews with current practitioners, Electronic Visual Music provides insight into contemporary working methods and gives a snapshot of the state of the art in this ever-evolving creative discipline. This book is a valuable resource for artists and practitioners, as well as students, educators and researchers working in disciplines such as music composition, music production, video arts, animation and related media arts, who are interested in informing their own work and learning new strategies and techniques for exploration and creative expression of electronic visual music.},
	urldate = {2025-02-11},
	publisher = {Taylor \& Francis Group},
	author = {Payling, Dave},
	year = {2023},
	keywords = {Mixed media (Music)-History and criticism., Mixed media (Music)-Performance., Mixed media (Music)-Production and direction.},
	file = {ProQuest Ebook Snapshot:/Users/giselaroberts/Zotero/storage/J8XBBPRB/detail.html:text/html},
}

@article{wenberg_-beatoff-beat_2021,
	title = {On-{Beat}/{Off}-{Beat} {Visual} {Responses} to {Audio}-{Visual} {Asynchrony} in {Music} {Videos}.},
	volume = {15},
	issn = {1934-9688},
	url = {https://research.ebsco.com/linkprocessor/plink?id=ca424a3a-e266-3888-bb23-563b58f7f894},
	doi = {10.3167/proj.2021.150103},
	abstract = {Audio-visual rhythm can be achieved in a variety of ways, in film as well as in music videos. Here, we have studied human visual responses to video editing with regard to musical beats, in order to better understand the role of visual rhythm in an audio-visual flow. While some suggest that music videos should maintain synchrony in the audio-visual rhythm, and others claim that music videos should be rhythmically loose in their structure, there is a functional aspect of vision and hearing that reacts to the juxtaposition of audio and visual rhythms. We present empirical evidence of cognitive effects, as well as perceptual differences with attentional effects, for viewers watching music videos cut on-beat and off-beat.},
	language = {eng},
	number = {1},
	urldate = {2025-02-11},
	journal = {Projections: The Journal for Movies \& Mind},
	author = {wenberg, Thorbjörn S and Carlgren, Simon},
	month = mar,
	year = {2021},
	note = {Publisher: Berghahn Books},
	keywords = {Motion picture music, Music videos, Synchronic order, Video editing},
	pages = {28--54},
	file = {Full text PDF:/Users/giselaroberts/Zotero/storage/ZFB3LMJU/wenberg and Carlgren - 2021 - On-BeatOff-Beat Visual Responses to Audio-Visual Asynchrony in Music Videos..pdf:application/pdf},
}

@inproceedings{li_pm4music_2024,
	address = {New York, NY, USA},
	series = {{ICDSP} '24},
	title = {{PM4Music}: {A} {Scriptable} {Parametric} {Modeling} {Interface} for {Music} {Visualizer} {Design} {Using} {PM4VR}},
	isbn = {979-8-4007-0902-9},
	shorttitle = {{PM4Music}},
	url = {https://dl.acm.org/doi/10.1145/3653876.3653906},
	doi = {10.1145/3653876.3653906},
	abstract = {The intersection of music and visual arts has long been a captivating field, offering immersive experiences that engage both auditory and visual senses. Music visualizers, in particular, serve as a dynamic medium for translating sound into captivating visual displays. This research paper introduces PM4Music, a cutting-edge scriptable parametric modeling interface tailored for music visualizer design. Leveraging the power of PM4VR, a versatile virtual reality environment, PM4Music empowers artists to create intricate and synchronized visualizations that enhance the auditory experience.},
	urldate = {2025-02-20},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Digital} {Signal} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Li, Wanwan},
	month = aug,
	year = {2024},
	pages = {33--38},
	file = {Full Text PDF:/Users/giselaroberts/Zotero/storage/8Z23BT7B/Li - 2024 - PM4Music A Scriptable Parametric Modeling Interface for Music Visualizer Design Using PM4VR.pdf:application/pdf},
}

@inproceedings{deja_vitune_2020,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '20},
	title = {{ViTune}: {A} {Visualizer} {Tool} to {Allow} the {Deaf} and {Hard} of {Hearing} to {See} {Music} {With} {Their} {Eyes}},
	isbn = {978-1-4503-6819-3},
	shorttitle = {{ViTune}},
	url = {https://dl.acm.org/doi/10.1145/3334480.3383046},
	doi = {10.1145/3334480.3383046},
	abstract = {Visualizers are usually added into music players to augment the listening experiences of hearing users. However, for the case of most members of the Deaf and Hard of Hearing (DHH) community, they have partial deafness which may give them a "limited" listening experience as compared to their counterparts. In this paper, we present ViTune, a visualizer tool that enhances the musical experiences of the DHH through the use of an on-screen visualizer generating effects alongside music. We observed how members of the DHH community typically experienced music through an initial user study. We then iterated on developing a visualizer prototype where we did multiple usability tests involving at least 15 participants from the DHH community. We observed how they experienced music with the help of our prototype and noticed that certain music features and elements augmented these experiences. Visualization attributes, their matching qualitative descriptions and equivalent subjective attributes were identified with the help of music experts. Also, affects hypothesized to be induced and dissuaded were identified in improving these listening experiences.},
	urldate = {2025-02-20},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Deja, Jordan Aiko and Dela Torre, Alexczar and Lee, Hans Joshua and Ciriaco, Jose Florencio and Eroles, Carlo Miguel},
	month = apr,
	year = {2020},
	pages = {1--8},
	file = {Full Text PDF:/Users/giselaroberts/Zotero/storage/P57LGBHB/Deja et al. - 2020 - ViTune A Visualizer Tool to Allow the Deaf and Hard of Hearing to See Music With Their Eyes.pdf:application/pdf},
}

@misc{noauthor_meaningful_nodate,
	title = {Meaningful {Music} {Visualizations} - {ProQuest}},
	url = {https://www.proquest.com/openview/6ac22100af685dc85d7135d8c90d2c0a/1?cbl=18750&diss=y&pq-origsite=gscholar},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	language = {en},
	urldate = {2025-02-20},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/33PN4IT4/1.html:text/html},
}

@inproceedings{le_real-time_2021,
	address = {New York, NY, USA},
	series = {{IAIT} '21},
	title = {Real-time {Sound} {Visualization} via {Multidimensional} {Clustering} and {Projections}},
	isbn = {978-1-4503-9012-5},
	url = {https://dl.acm.org/doi/10.1145/3468784.3471604},
	doi = {10.1145/3468784.3471604},
	abstract = {Sound plays a vital role in every aspect of human life since it is one of the primary sensory information that our auditory system collects and allows us to perceive the world. Sound clustering and visualization is the process of collecting and analyzing audio samples; that process is a prerequisite of sound classification, which is the core of automatic speech recognition, virtual assistants, and text to speech applications. Nevertheless, understanding how to recognize and properly interpret complex, high-dimensional audio data is the most significant challenge in sound clustering and visualization. This paper proposed a web-based platform to visualize and cluster similar sound samples of musical notes and human speech in real-time. For visualizing high-dimensional data like audio, Mel-Frequency Cepstral Coefficients (MFCCs) were initially developed to represent the sounds made by the human vocal tract are extracted. Then, t-distributed Stochastic Neighbor Embedding (t-SNE), a dimensionality reduction technique, was designed for high dimensional datasets is applied. This paper focuses on both data clustering and high-dimensional visualization methods to properly present the clustering results in the most meaningful way to uncover potentially interesting behavioral patterns of musical notes played by different instruments.},
	urldate = {2025-02-21},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Advances} in {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Le, Nhat and Nguyen, Ngan V.T. and Dang, Tommy},
	month = jul,
	year = {2021},
	pages = {1--6},
}

@book{smus_web_2013,
	title = {Web {Audio} {API}: {Advanced} {Sound} for {Games} and {Interactive} {Apps}},
	isbn = {978-1-4493-3264-8},
	shorttitle = {Web {Audio} {API}},
	abstract = {Go beyond HTML5’s Audio tag and boost the audio capabilities of your web application with the Web Audio API. Packed with lots of code examples, crisp descriptions, and useful illustrations, this concise guide shows you how to use this JavaScript API to make the sounds and music of your games and interactive applications come alive.You need little or no digital audio expertise to get started. Author Boris Smus introduces you to digital audio concepts, then shows you how the Web Audio API solves specific application audio problems. If you're an experienced JavaScript programmer, you’ll not only learn how to synthesize and process digital audio, you’ll also explore audio analysis and visualization with this API.Learn Web Audio API, including audio graphs and the audio nodesProvide quick feedback to user actions by scheduling sounds with the API’s precise timing modelControl gain, volume, and loudness, and dive into clipping and crossfadingUnderstand pitch and frequency: use tools to manipulate soundforms directly with JavaScriptGenerate synthetic sound effects and learn how to spatialize sound in 3D spaceUse Web Audio API with the Audio tag, getUserMedia, and the Page Visibility API},
	language = {en},
	publisher = {"O'Reilly Media, Inc."},
	author = {Smus, Boris},
	month = mar,
	year = {2013},
	note = {Google-Books-ID: eSPyRuL8b7UC},
	keywords = {Computers / Design, Graphics \& Media / Audio, Computers / Internet / Podcasting \& Webcasting, Computers / Internet / Web Services \& APIs},
}

@misc{noauthor_book_nodate,
	title = {The {Book} of {Shaders}},
	url = {https://thebookofshaders.com/},
	abstract = {Gentle step-by-step guide through the abstract and complex universe of Fragment Shaders.},
	urldate = {2025-02-24},
	journal = {The Book of Shaders},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/LD2UMH6I/thebookofshaders.com.html:text/html},
}

@misc{noauthor_web_nodate,
	title = {Web {Audio} {API} - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API},
	urldate = {2025-02-24},
	file = {Web Audio API - Web APIs | MDN:/Users/giselaroberts/Zotero/storage/G2UGA6VS/Web_Audio_API.html:text/html},
}

@misc{noauthor_web_nodate-1,
	title = {Web {Audio} {API} - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API},
	urldate = {2025-02-24},
	file = {Web Audio API - Web APIs | MDN:/Users/giselaroberts/Zotero/storage/BEEXPPGF/Web_Audio_API.html:text/html},
}

@misc{noauthor_web_2025,
	title = {Web {Audio} {API} - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API},
	abstract = {The Web Audio API provides a powerful and versatile system for controlling audio on the Web, allowing developers to choose audio sources, add effects to audio, create audio visualizations, apply spatial effects (such as panning) and much more.},
	language = {en-US},
	urldate = {2025-02-24},
	month = feb,
	year = {2025},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/B3P9USFU/Web_Audio_API.html:text/html},
}

@misc{noauthor_canvas_2024,
	title = {Canvas {API} - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API},
	abstract = {The Canvas API provides a means for drawing graphics via JavaScript and the HTML {\textless}canvas{\textgreater} element. Among other things, it can be used for animation, game graphics, data visualization, photo manipulation, and real-time video processing.},
	language = {en-US},
	urldate = {2025-02-24},
	month = aug,
	year = {2024},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/GSGY4IHG/Canvas_API.html:text/html},
}

@misc{noauthor_webgl_nodate,
	title = {{WebGL}: {2D} and {3D} graphics for the web - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API},
	urldate = {2025-02-24},
	file = {WebGL\: 2D and 3D graphics for the web - Web APIs | MDN:/Users/giselaroberts/Zotero/storage/DCWRJIH3/WebGL_API.html:text/html},
}

@phdthesis{usher_visualization_2023,
	type = {thesis},
	title = {Visualization of {Sounds}: {Improving} the {Listening} {Experience} {Through} {Digital} {Visualization}},
	shorttitle = {Visualization of {Sounds}},
	url = {https://rshare.library.torontomu.ca/articles/thesis/Visualization_of_Sounds_Improving_the_Listening_Experience_Through_Digital_Visualization/23159858/1},
	abstract = {In this project we will observe the persuasiveness of visualization when pertaining to audio data. Specifically, we look at instrumental music composition and observe how visualization can facilitate the communication of a music-maker's state of mind. We first examine, through a research process, how visualization can facilitate the processing of information. Then, steps are taken using data bending and digital data visualization techniques – with minimal creative input – to generate animated visuals from the audio data. Finally, the digital visualizations are introduced to participants via a survey to measure how effective these visualizations were at translating intentionality.},
	language = {en},
	urldate = {2025-02-28},
	school = {Toronto Metropolitan University},
	author = {Usher, Maeliss E.},
	month = may,
	year = {2023},
	doi = {10.32920/23159858.v1},
	file = {Full Text PDF:/Users/giselaroberts/Zotero/storage/K3A97GWP/Usher - 2023 - Visualization of Sounds Improving the Listening Experience Through Digital Visualization.pdf:application/pdf},
}

@article{gupta_distraction_2019,
	title = {Distraction during cystoscopy to reduce pain and increase satisfaction: {Randomized} control study between real-time visualization versus listening to music versus combined music and real-time visualization},
	volume = {11},
	issn = {0974-7796},
	shorttitle = {Distraction during cystoscopy to reduce pain and increase satisfaction},
	url = {https://journals.lww.com/urol/fulltext/2019/11010/Distraction_during_cystoscopy_to_reduce_pain_and.6.aspx},
	doi = {10.4103/UA.UA_191_17},
	abstract = {Purpose: 
          This study aims to compare the various distraction methods used during office cystoscopy to decrease pain and dissatisfaction among patients.
          Materials and Methods: 
          Two hundred patients undergoing rigid cystoscopy between January 2017 and July 2017 were randomized into four groups of 50 patients: (1) Group I: Patients who listened to music during the cystoscopy, (2) Group II: Patients allowed real-time visualization of the cystoscopy, (3) Group III: Patients who listened to music and had real-time visualization of the procedure, (4) Group IV: Control group undergoing cystoscopy without any distraction used. A visual analog scale (VAS) (1–10) was used for a self-assessment of pain, satisfaction, and willingness for repeat cystoscopy.
          Results: 
          Demographic characteristics, mean age, procedure duration, and procedure indications were statistically similar between the four groups. The mean VAS pain score were significantly lower in the three study Groups (I, II, and III) where distraction methods were used during cystoscopies as compared to the control Group IV (P {\textless} 0.001) and the satisfaction VAS scores and VAS scores for willingness to undergo a repeat procedure were significantly higher in the study groups (P {\textless} 0.001). Statistically significant decreased postprocedural pulse rate and blood pressure in comparison with to their preprocedural values were observed when distraction methods were used (P {\textless} 0.01). Patients undergoing cystoscopies listening to music and real-time visualization (Group III) had better VAS scores than the others (P {\textless} 0.01).
          Conclusions: 
          Distraction methods reduce pain and increase satisfaction among patients. Best results are with combined listening to music and direct real-time visualization.},
	language = {en-US},
	number = {1},
	urldate = {2025-02-28},
	journal = {Urology Annals},
	author = {Gupta, Sandeep and Das, Susanta Kumar and Jana, Debarshi and Pal, Dilip Kumar},
	month = mar,
	year = {2019},
	pages = {33},
	file = {PubMed Central Full Text PDF:/Users/giselaroberts/Zotero/storage/55MHWIR6/Gupta et al. - 2019 - Distraction during cystoscopy to reduce pain and increase satisfaction Randomized control study bet.pdf:application/pdf;Snapshot:/Users/giselaroberts/Zotero/storage/CX66CQJE/Distraction_during_cystoscopy_to_reduce_pain_and.6.html:text/html},
}

@misc{noauthor_chrome_nodate,
	title = {Chrome {Music} {Lab}},
	url = {https://musiclab.chromeexperiments.com},
	abstract = {Music is for everyone. Play with simple experiments that let anyone, of any age, explore how music works.},
	language = {en},
	urldate = {2025-03-01},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/HVSF3DWS/About.html:text/html},
}

@misc{noauthor_geisswerks_nodate,
	title = {Geisswerks - {About} {MilkDrop}},
	url = {https://www.geisswerks.com/about_milkdrop.html},
	urldate = {2025-03-01},
	file = {Geisswerks - About MilkDrop:/Users/giselaroberts/Zotero/storage/QEXYVPM2/about_milkdrop.html:text/html},
}
@misc{QuickCodingTuts,
        title = {Building a Dynamic JavaScript Audio Visualizer with the Web Audio API},
        url = {https://www.youtube.com/watch?v=sb6C1XNqJzA},
        urldate = {2023-06-15}
}
@misc{adminofthissite,
        title = {20Hz to 20kHz (Human Audio Spectrum)},
        url = {https://www.youtube.com/watch?v=qNf9nzvnd1k},
        urldate = {2012-10-12}
}
@misc{ADMIs,
	author = {Duarte, Erivan Goncalves and Cossette, Isabel and Wanderley, Marcelo M.},
	title = {Analysis of Accessible Digital Musical Instruments through the lens of Disability Models},
	url = {\url{https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1158476/full#B39}},
	month = oct,
        year = {2023},
	note = {[Accessed 25-04-2025]},
}

@book{mitYoungPeople,
    author = {James, Carrie and Davis, Katie and Flores, Andrea and Francis, John M. and Pettingill, Lindsay and Rundle, Margaret and Gardner, Howard},
    title = {{Y}oung {P}eople, {E}thics, and the {N}ew {D}igital {M}edia --- mitpress.mit.edu},
    publisher = {\url{https://mitpress.mit.edu/9780262513630/young-people-ethics-and-the-new-digital-media/}},
    month = oct,
    year = {2009}
}

@misc{nihEthicsDigital,
	author = {Burr, Christopher and Taddeo, Mariarosaria and Floridi, Luciano},
	title = {{T}he {E}thics of {D}igital {W}ell-{B}eing: {A} {T}hematic {R}eview --- pmc.ncbi.nlm.nih.gov},
	howpublished = {\url{https://pmc.ncbi.nlm.nih.gov/articles/PMC7417400/}},
	month = jan,
        year = {2020},
	note = {[Accessed 25-04-2025]},
}

@book{ProQuest,
    author = {Chiodo, Simona and Kaiser, David and Shah, Julie and Volonte, Paolo},
    title = {Improving Technology Through Ethics} ,
    publisher = Springer,
    month = feb,
    year = {2024}
}

@article{UsingTechnologyNurses,
    author = {Webster, Natalie L. and Oyebode, Jan R. and Jenking, Catharine and Bicknell, Sarah and Smythe, Analisa},
    title = {Using technology to support the emotional and social well-being of nurses: A scoping review},
    journal = {Journal of Advanced Nursing VOlume 76 Issue 1} ,
    month = oct,
    year = {2019}
}


@article{collins_algorave_nodate,
	title = {Algorave: {Live} {Performance} of {Algorithmic} {Electronic} {Dance} {Music}},
	abstract = {The algorave movement has received international exposure in the last two years, including a series of concerts in Europe and beyond, and press coverage in a number of media. This paper seeks to illuminate some of the historical precedents to the scene, its primary aesthetic goals, and the divergent technological and musical approaches of representative participants. We keep in mind the novel possibilities in musical expression explored by algoravers. The scene is by no means homogeneous, and the very lack of uniformity of technique, from new live coding languages through code DJing to plug-in combination, with or without visual extension, is indicative of the ﬂexibility of computers themselves as general information processors.},
	language = {en},
	author = {Collins, Nick and McLean, Alex},
	file = {PDF:/Users/giselaroberts/Zotero/storage/GVYEDLLI/Collins and McLean - Algorave Live Performance of Algorithmic Electronic Dance Music.pdf:application/pdf},
}

@article{diapoulis_reproducible_2023,
	title = {Reproducible {Musical} {Analysis} of {Live} {Coding} {Performances} {Using} {Information} {Retrieval}: {A} {Case} {Study} on the {Algorave} 10th {Anniversary}},
	copyright = {Creative Commons Attribution 4.0 International, Open Access},
	shorttitle = {Reproducible {Musical} {Analysis} of {Live} {Coding} {Performances} {Using} {Information} {Retrieval}},
	url = {https://zenodo.org/record/7843813},
	doi = {10.5281/ZENODO.7843813},
	abstract = {We present a reproducible music information retrieval (MIR) study on 133 performances from the 10th anniversary of Algorave. Our aim in this paper is to provide a reproducible framework for computational analysis of musical performances. Here, we present a tool for analysing acoustical characteristics and for visualizing the musical structure from performances of one algorave event. Our musical analysis of the live coding performances highlights the musical diversity within the live coding community to a broader scientific audience. At the same time, we expect that the algoravers will gain insights on their own musical practices through the computational analysis of the musical structure of their performances. In concerning ourselves with reproducibility, our intention is to motivate more researchers to analyse musical practices of other under-represented music communities. As a basic tool for reproducibility we construct a pipeline for analysing performances using Python within a Jupyter notebook. To make this reproducible on different computers we wrapped the whole workflow setup into a docker image. We represent the results of our analysis as a series of plots of different kinds. These plots present both overviews of the entire repertory in compact form, and comparisons of individual pieces in more detail. In learning one can use such visualization as a means for raising awareness on one’s evolution of the musical outcome. In performance this visualization can be developed to a real-time and possibly an interactive tool which informs the coder about the musical outcome of a live set on-the-fly. Finally, we reflect on how and to what extent such MIR studies can provide valuable insights in live coding performance practices, while also considering the limitations faced when dealing with such large parameter spaces in human machine musicianship.},
	language = {en},
	urldate = {2025-01-30},
	author = {Diapoulis, Georgios and Carlé, Martin},
	month = apr,
	year = {2023},
	note = {Publisher: Zenodo},
	annote = {Other
This contribution has been partially funded through the financial support of the project "ΔΗΜΙΟΥΡΓΙΚΟΣ ΚΟΜΒΟΣ ΤΕΧΝΩΝ MIS :5047267" code 80504, ΕΣΠΑ 2014-2020, ΕΠΑνΕΚ; HAL (Hub of Art Laboratories), co-financed by Greece and the European Union and implemented at the Ionian University, Corfu.},
	file = {PDF:/Users/giselaroberts/Zotero/storage/RMCWY5PR/Diapoulis and Carlé - 2023 - Reproducible Musical Analysis of Live Coding Performances Using Information Retrieval A Case Study.pdf:application/pdf},
}

@inproceedings{noauthor_music_2018,
	title = {Music {Computer} {Technologies} in {Computer} {Science} and {Music} {Studies} at {Schools} for {Children} with {Deep} {Visual} {Impairment}},
	isbn = {978-93-86878-23-6},
	url = {http://uruae.org/siteadmin/upload/3920UH10184021.pdf},
	doi = {10.17758/URUAE4.UH10184021},
	abstract = {The article analyzes the processes of information, transforming the environment of training of pupil with visual impairment. There is a necessity to change the content of the information education in connection with the use of specialized software, and hardware and digital educational resources. Special rehabilitation centers that teach computer science and contemporary information technology (IT) and allow student with visual impairment to adapt to the development and independent use of specialized IT, help in solving a number of important problems. The tiflotechnik class implements contemporary technological capabilities for training visually impaired information and music computer technologies. Analysis of the peculiarities of the use of music computer technologies (MCT) by rehabilitators with visual impairments shows that currently blind musicians have the opportunity to master a number of MCT-programs (sequencers, audio editors, etc.), which contributes to the most profound disclosure of their creative potential.},
	language = {en},
	urldate = {2025-01-31},
	booktitle = {{TSETWM}-18,{CAAES}-18,{LLHSS}-18,{LEBDM}-18 {Oct}. 2-4, 2018 {Budapest} ({Hungary})},
	publisher = {Universal Researchers},
	month = oct,
	year = {2018},
	file = {PDF:/Users/giselaroberts/Zotero/storage/5ZFXMH88/2018 - Music Computer Technologies in Computer Science and Music Studies at Schools for Children with Deep.pdf:application/pdf},
}

@article{dannenberg_languages_2018,
	title = {Languages for {Computer} {Music}},
	volume = {5},
	issn = {2297-2668},
	url = {https://www.frontiersin.org/journals/digital-humanities/articles/10.3389/fdigh.2018.00026/full},
	doi = {10.3389/fdigh.2018.00026},
	abstract = {{\textless}p{\textgreater}Specialized languages for computer music have long been an important area of research in this community. Computer music languages have enabled composers who are not software engineers to nevertheless use computers effectively. While powerful general-purpose programming languages can be used for music tasks, experience has shown that time plays a special role in music computation, and languages that embrace musical time are especially expressive for many musical tasks. Time is expressed in procedural languages through schedulers and abstractions of beats, duration and tempo. Functional languages have been extended with temporal semantics, and object-oriented languages are often used to model stream-based computation of audio. This article considers models of computation that are especially important for music programming, how these models are supported in programming languages, and how this leads to expressive and efficient programs. Concrete examples are drawn from some of the most widely used music programming languages.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-01-31},
	journal = {Frontiers in Digital Humanities},
	author = {Dannenberg, Roger B.},
	month = nov,
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {functional programming, music languages, Music representations, Object-oriented programming, Real-time, sound synthesis, Visual programming},
	file = {Full Text PDF:/Users/giselaroberts/Zotero/storage/X7PRW6KG/Dannenberg - 2018 - Languages for Computer Music.pdf:application/pdf},
}

@article{mcalpine_making_1999,
	title = {Making {Music} with {Algorithms}: {A} {Case}-{Study} {System}},
	volume = {23},
	issn = {0148-9267},
	shorttitle = {Making {Music} with {Algorithms}},
	url = {https://www.jstor.org/stable/3680733},
	number = {2},
	urldate = {2025-01-31},
	journal = {Computer Music Journal},
	author = {McAlpine, Kenneth and Miranda, Eduardo and Hoggar, Stuart},
	year = {1999},
	note = {Publisher: The MIT Press},
	pages = {19--30},
	file = {JSTOR Full Text PDF:/Users/giselaroberts/Zotero/storage/YNNRY6RH/McAlpine et al. - 1999 - Making Music with Algorithms A Case-Study System.pdf:application/pdf},
}

@phdthesis{cataldi_1990-_end_2022,
	type = {Thesis},
	title = {The end of {Graphical} {Sequencers} : {TidalCycles} as a way of experimentation},
	shorttitle = {The end of {Graphical} {Sequencers}},
	url = {https://skemman.is/handle/1946/43349},
	abstract = {As most musicians who have experience with digital tools already know, commercial Digital Audio Workstations (DAWs) and graphical sequencers, are rigid tools when it comes to experimenting with complex sequences and simultaneous parameter changes. Moreover, these environments silently influence the digital music composer’s aesthetic decision and suggest specific working methodologies with their innate structure and interface design. Most of the time musicians will have to unconsciously alter their way of working for the ones suggested by the music software, embracing its conceptual and compositional constraints. this meaning that musicians will have their musical expression outlined by the piece of music software they utilize. This being said, there are tools that will shape the musician’s mind-set in a more pronounced way than others.The purpose of this essay is to compare Graphical Sequencers with text-based programming environments. I chose to analyze open text-based programming environments; thus, they allow a more extensive experimentation, simultaneous parameters control, complex sequence creation and sophisticated audio manipulation than graphical sequencers. In addition, text- based programming tools have no fixed user interface, the environments start up with a blank page a tabula rasa, having less influence over the musicians and allowing them more room for experimentation.Among the wide scale of existing music programing tools, I will be mainly focusing my thesis on the analysis of TidalCycle as a case study. I will introduce the program basics, its inner architecture, how to structure and deal with pattern creation and also describe its approach to musical time.Through this essay I’ll examine the usability problem of graphical sequencers, the suggestion of technology on musician’s workflow and the influence of graphical interfaces. Conceptual examples and use of actual code will be presented in this essay to illustrate particular cases. Finally, I will present the conclusions reached by the analysis of this subject.},
	language = {en},
	urldate = {2025-01-31},
	author = {Cataldi 1990-, Gustavo Nicolás Villanueva},
	month = may,
	year = {2022},
	note = {Accepted: 2023-01-27T13:58:31Z},
	file = {Full Text PDF:/Users/giselaroberts/Zotero/storage/A4G2SPCV/Cataldi 1990- - 2022 - The end of Graphical Sequencers  TidalCycles as a way of experimentation.pdf:application/pdf},
}

@misc{noauthor_what_nodate,
	title = {what is firespring - {Google} {Search}},
	url = {https://www.google.com/search?q=what+is+firespring&sca_esv=84d59d8bbfc4078c&sxsrf=AHTn8zpOY_uDYmIvNoPkbV-FZjzFQ9smvA%3A1738371593360&ei=CXKdZ-fbFc2wur8PyuDn0QY&ved=0ahUKEwinhL3qoqGLAxVNmO4BHUrwOWoQ4dUDCBA&uact=5&oq=what+is+firespring&gs_lp=Egxnd3Mtd2l6LXNlcnAiEndoYXQgaXMgZmlyZXNwcmluZzILEAAYgAQYkQIYigUyBhAAGBYYHjIFEAAY7wUyBRAAGO8FMggQABiABBiiBDIIEAAYgAQYogRIvxVQmAVYtxRwBHgBkAEAmAGfAaABiAyqAQQxNC40uAEDyAEA-AEBmAIWoAKPDcICChAAGLADGNYEGEfCAgQQIxgnwgIKECMYgAQYJxiKBcICCxAAGIAEGLEDGIMBwgIOEC4YgAQYsQMY0QMYxwHCAhEQLhiABBixAxjRAxiDARjHAcICCxAuGIAEGLEDGNQCwgIKEAAYgAQYQxiKBcICEBAAGIAEGLEDGEMYgwEYigXCAg4QABiABBixAxiDARiKBcICCBAAGIAEGLEDwgIFEAAYgATCAhEQABiABBiRAhixAxiDARiKBcICBBAAGAPCAggQABgWGAoYHsICCxAAGIAEGIYDGIoFmAMAiAYBkAYFkgcEMTcuNaAH1YIB&sclient=gws-wiz-serp},
	urldate = {2025-02-01},
	file = {what is firespring - Google Search:/Users/giselaroberts/Zotero/storage/9B9923TZ/search.html:text/html},
}

@misc{noauthor_computer_nodate,
	title = {{COMPUTER} {RECREATIONS} on {JSTOR}},
	url = {https://www.jstor.org/stable/24969383},
	urldate = {2025-02-01},
	file = {COMPUTER RECREATIONS on JSTOR:/Users/giselaroberts/Zotero/storage/434Z6WV6/24969383.html:text/html},
}

@book{payling_electronic_2023,
	address = {Oxford, UNITED KINGDOM},
	title = {Electronic {Visual} {Music}: {The} {Elements} of {Audiovisual} {Creativity}},
	isbn = {978-1-000-93647-6},
	shorttitle = {Electronic {Visual} {Music}},
	url = {http://ebookcentral.proquest.com/lib/oxy/detail.action?docID=7271293},
	abstract = {Electronic Visual Music is a comprehensive guide to the composition and performance of visual music, and an essential text for those wanting to explore the history, current practice, performance strategies, compositional methodologies and practical techniques for conceiving and creating electronic visual music. Beginning with historical perspectives to inspire the reader to work creatively and develop their own individual style, visual music theory is then discussed in an accessible form, providing a series of strategies for implementing ideas. Including interviews with current practitioners, Electronic Visual Music provides insight into contemporary working methods and gives a snapshot of the state of the art in this ever-evolving creative discipline. This book is a valuable resource for artists and practitioners, as well as students, educators and researchers working in disciplines such as music composition, music production, video arts, animation and related media arts, who are interested in informing their own work and learning new strategies and techniques for exploration and creative expression of electronic visual music.},
	urldate = {2025-02-11},
	publisher = {Taylor \& Francis Group},
	author = {Payling, Dave},
	year = {2023},
	keywords = {Mixed media (Music)-History and criticism., Mixed media (Music)-Performance., Mixed media (Music)-Production and direction.},
	file = {ProQuest Ebook Snapshot:/Users/giselaroberts/Zotero/storage/J8XBBPRB/detail.html:text/html},
}

@article{wenberg_-beatoff-beat_2021,
	title = {On-{Beat}/{Off}-{Beat} {Visual} {Responses} to {Audio}-{Visual} {Asynchrony} in {Music} {Videos}.},
	volume = {15},
	issn = {1934-9688},
	url = {https://research.ebsco.com/linkprocessor/plink?id=ca424a3a-e266-3888-bb23-563b58f7f894},
	doi = {10.3167/proj.2021.150103},
	abstract = {Audio-visual rhythm can be achieved in a variety of ways, in film as well as in music videos. Here, we have studied human visual responses to video editing with regard to musical beats, in order to better understand the role of visual rhythm in an audio-visual flow. While some suggest that music videos should maintain synchrony in the audio-visual rhythm, and others claim that music videos should be rhythmically loose in their structure, there is a functional aspect of vision and hearing that reacts to the juxtaposition of audio and visual rhythms. We present empirical evidence of cognitive effects, as well as perceptual differences with attentional effects, for viewers watching music videos cut on-beat and off-beat.},
	language = {eng},
	number = {1},
	urldate = {2025-02-11},
	journal = {Projections: The Journal for Movies \& Mind},
	author = {wenberg, Thorbjörn S and Carlgren, Simon},
	month = mar,
	year = {2021},
	note = {Publisher: Berghahn Books},
	keywords = {Motion picture music, Music videos, Synchronic order, Video editing},
	pages = {28--54},
	file = {Full text PDF:/Users/giselaroberts/Zotero/storage/ZFB3LMJU/wenberg and Carlgren - 2021 - On-BeatOff-Beat Visual Responses to Audio-Visual Asynchrony in Music Videos..pdf:application/pdf},
}

@inproceedings{li_pm4music_2024,
	address = {New York, NY, USA},
	series = {{ICDSP} '24},
	title = {{PM4Music}: {A} {Scriptable} {Parametric} {Modeling} {Interface} for {Music} {Visualizer} {Design} {Using} {PM4VR}},
	isbn = {979-8-4007-0902-9},
	shorttitle = {{PM4Music}},
	url = {https://dl.acm.org/doi/10.1145/3653876.3653906},
	doi = {10.1145/3653876.3653906},
	abstract = {The intersection of music and visual arts has long been a captivating field, offering immersive experiences that engage both auditory and visual senses. Music visualizers, in particular, serve as a dynamic medium for translating sound into captivating visual displays. This research paper introduces PM4Music, a cutting-edge scriptable parametric modeling interface tailored for music visualizer design. Leveraging the power of PM4VR, a versatile virtual reality environment, PM4Music empowers artists to create intricate and synchronized visualizations that enhance the auditory experience.},
	urldate = {2025-02-20},
	booktitle = {Proceedings of the 2024 8th {International} {Conference} on {Digital} {Signal} {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Li, Wanwan},
	month = aug,
	year = {2024},
	pages = {33--38},
	file = {Full Text PDF:/Users/giselaroberts/Zotero/storage/8Z23BT7B/Li - 2024 - PM4Music A Scriptable Parametric Modeling Interface for Music Visualizer Design Using PM4VR.pdf:application/pdf},
}

@inproceedings{deja_vitune_2020,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '20},
	title = {{ViTune}: {A} {Visualizer} {Tool} to {Allow} the {Deaf} and {Hard} of {Hearing} to {See} {Music} {With} {Their} {Eyes}},
	isbn = {978-1-4503-6819-3},
	shorttitle = {{ViTune}},
	url = {https://dl.acm.org/doi/10.1145/3334480.3383046},
	doi = {10.1145/3334480.3383046},
	abstract = {Visualizers are usually added into music players to augment the listening experiences of hearing users. However, for the case of most members of the Deaf and Hard of Hearing (DHH) community, they have partial deafness which may give them a "limited" listening experience as compared to their counterparts. In this paper, we present ViTune, a visualizer tool that enhances the musical experiences of the DHH through the use of an on-screen visualizer generating effects alongside music. We observed how members of the DHH community typically experienced music through an initial user study. We then iterated on developing a visualizer prototype where we did multiple usability tests involving at least 15 participants from the DHH community. We observed how they experienced music with the help of our prototype and noticed that certain music features and elements augmented these experiences. Visualization attributes, their matching qualitative descriptions and equivalent subjective attributes were identified with the help of music experts. Also, affects hypothesized to be induced and dissuaded were identified in improving these listening experiences.},
	urldate = {2025-02-20},
	booktitle = {Extended {Abstracts} of the 2020 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Deja, Jordan Aiko and Dela Torre, Alexczar and Lee, Hans Joshua and Ciriaco, Jose Florencio and Eroles, Carlo Miguel},
	month = apr,
	year = {2020},
	pages = {1--8},
	file = {Full Text PDF:/Users/giselaroberts/Zotero/storage/P57LGBHB/Deja et al. - 2020 - ViTune A Visualizer Tool to Allow the Deaf and Hard of Hearing to See Music With Their Eyes.pdf:application/pdf},
}

@misc{noauthor_meaningful_nodate,
	title = {Meaningful {Music} {Visualizations} - {ProQuest}},
	url = {https://www.proquest.com/openview/6ac22100af685dc85d7135d8c90d2c0a/1?cbl=18750&diss=y&pq-origsite=gscholar},
	abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
	language = {en},
	urldate = {2025-02-20},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/33PN4IT4/1.html:text/html},
}

@inproceedings{le_real-time_2021,
	address = {New York, NY, USA},
	series = {{IAIT} '21},
	title = {Real-time {Sound} {Visualization} via {Multidimensional} {Clustering} and {Projections}},
	isbn = {978-1-4503-9012-5},
	url = {https://dl.acm.org/doi/10.1145/3468784.3471604},
	doi = {10.1145/3468784.3471604},
	abstract = {Sound plays a vital role in every aspect of human life since it is one of the primary sensory information that our auditory system collects and allows us to perceive the world. Sound clustering and visualization is the process of collecting and analyzing audio samples; that process is a prerequisite of sound classification, which is the core of automatic speech recognition, virtual assistants, and text to speech applications. Nevertheless, understanding how to recognize and properly interpret complex, high-dimensional audio data is the most significant challenge in sound clustering and visualization. This paper proposed a web-based platform to visualize and cluster similar sound samples of musical notes and human speech in real-time. For visualizing high-dimensional data like audio, Mel-Frequency Cepstral Coefficients (MFCCs) were initially developed to represent the sounds made by the human vocal tract are extracted. Then, t-distributed Stochastic Neighbor Embedding (t-SNE), a dimensionality reduction technique, was designed for high dimensional datasets is applied. This paper focuses on both data clustering and high-dimensional visualization methods to properly present the clustering results in the most meaningful way to uncover potentially interesting behavioral patterns of musical notes played by different instruments.},
	urldate = {2025-02-21},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Advances} in {Information} {Technology}},
	publisher = {Association for Computing Machinery},
	author = {Le, Nhat and Nguyen, Ngan V.T. and Dang, Tommy},
	month = jul,
	year = {2021},
	pages = {1--6},
}

@book{smus_web_2013,
	title = {Web {Audio} {API}: {Advanced} {Sound} for {Games} and {Interactive} {Apps}},
	isbn = {978-1-4493-3264-8},
	shorttitle = {Web {Audio} {API}},
	abstract = {Go beyond HTML5’s Audio tag and boost the audio capabilities of your web application with the Web Audio API. Packed with lots of code examples, crisp descriptions, and useful illustrations, this concise guide shows you how to use this JavaScript API to make the sounds and music of your games and interactive applications come alive.You need little or no digital audio expertise to get started. Author Boris Smus introduces you to digital audio concepts, then shows you how the Web Audio API solves specific application audio problems. If you're an experienced JavaScript programmer, you’ll not only learn how to synthesize and process digital audio, you’ll also explore audio analysis and visualization with this API.Learn Web Audio API, including audio graphs and the audio nodesProvide quick feedback to user actions by scheduling sounds with the API’s precise timing modelControl gain, volume, and loudness, and dive into clipping and crossfadingUnderstand pitch and frequency: use tools to manipulate soundforms directly with JavaScriptGenerate synthetic sound effects and learn how to spatialize sound in 3D spaceUse Web Audio API with the Audio tag, getUserMedia, and the Page Visibility API},
	language = {en},
	publisher = {"O'Reilly Media, Inc."},
	author = {Smus, Boris},
	month = mar,
	year = {2013},
	note = {Google-Books-ID: eSPyRuL8b7UC},
	keywords = {Computers / Design, Graphics \& Media / Audio, Computers / Internet / Podcasting \& Webcasting, Computers / Internet / Web Services \& APIs},
}

@misc{noauthor_book_nodate,
	title = {The {Book} of {Shaders}},
	url = {https://thebookofshaders.com/},
	abstract = {Gentle step-by-step guide through the abstract and complex universe of Fragment Shaders.},
	urldate = {2025-02-24},
	journal = {The Book of Shaders},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/LD2UMH6I/thebookofshaders.com.html:text/html},
}

@misc{noauthor_web_nodate,
	title = {Web {Audio} {API} - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API},
	urldate = {2025-02-24},
	file = {Web Audio API - Web APIs | MDN:/Users/giselaroberts/Zotero/storage/G2UGA6VS/Web_Audio_API.html:text/html},
}

@misc{noauthor_web_nodate-1,
	title = {Web {Audio} {API} - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API},
	urldate = {2025-02-24},
	file = {Web Audio API - Web APIs | MDN:/Users/giselaroberts/Zotero/storage/BEEXPPGF/Web_Audio_API.html:text/html},
}

@misc{noauthor_web_2025,
	title = {Web {Audio} {API} - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API},
	abstract = {The Web Audio API provides a powerful and versatile system for controlling audio on the Web, allowing developers to choose audio sources, add effects to audio, create audio visualizations, apply spatial effects (such as panning) and much more.},
	language = {en-US},
	urldate = {2025-02-24},
	month = feb,
	year = {2025},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/B3P9USFU/Web_Audio_API.html:text/html},
}

@misc{noauthor_canvas_2024,
	title = {Canvas {API} - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/Canvas_API},
	abstract = {The Canvas API provides a means for drawing graphics via JavaScript and the HTML {\textless}canvas{\textgreater} element. Among other things, it can be used for animation, game graphics, data visualization, photo manipulation, and real-time video processing.},
	language = {en-US},
	urldate = {2025-02-24},
	month = aug,
	year = {2024},
	file = {Snapshot:/Users/giselaroberts/Zotero/storage/GSGY4IHG/Canvas_API.html:text/html},
}

@misc{noauthor_webgl_nodate,
	title = {{WebGL}: {2D} and {3D} graphics for the web - {Web} {APIs} {\textbar} {MDN}},
	url = {https://developer.mozilla.org/en-US/docs/Web/API/WebGL_API},
	urldate = {2025-02-24},
	file = {WebGL\: 2D and 3D graphics for the web - Web APIs | MDN:/Users/giselaroberts/Zotero/storage/DCWRJIH3/WebGL_API.html:text/html},
}
